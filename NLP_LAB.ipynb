{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Parts-Of-Speech taggeing in NLP"
      ],
      "metadata": {
        "id": "4VPA7miP3XjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using NLTK library"
      ],
      "metadata": {
        "id": "ql9fqYvI3pbh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piCjD8Cximhx",
        "outputId": "858c1217-93ff-47e7-b0ce-ec4d40500c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ram', 'NNP'),\n",
              " ('ate', 'VBP'),\n",
              " ('the', 'DT'),\n",
              " ('delicious', 'JJ'),\n",
              " ('chocolate', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text=\"Ram ate the delicious chocolate\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "res=word_tokenize(text)\n",
        "nltk.pos_tag(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Spacy library"
      ],
      "metadata": {
        "id": "5LmOv7P_3x65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(text)\n",
        "for word in doc:\n",
        "  print(word,\"|\",word.pos_,\"|\",spacy.explain(word.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTOtxmDBi8av",
        "outputId": "0928cea9-05c2-4773-bd50-5ade6dcd4658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ram | NOUN | noun\n",
            "ate | VERB | verb\n",
            "the | DET | determiner\n",
            "delicious | ADJ | adjective\n",
            "chocolate | NOUN | noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D0eWPYEIkFm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Perform Word Tokenization"
      ],
      "metadata": {
        "id": "VAAqnlQ54aDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "text='''He said \"i need delicious chocolate.\" shopkeeper replied \"That is not good, This is better!\"'''\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyrZuIhs4bF-",
        "outputId": "fdcfe89b-d80c-4864-db7a-1bb60b1504e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He',\n",
              " 'said',\n",
              " '``',\n",
              " 'i',\n",
              " 'need',\n",
              " 'delicious',\n",
              " 'chocolate',\n",
              " '.',\n",
              " \"''\",\n",
              " 'shopkeeper',\n",
              " 'replied',\n",
              " '``',\n",
              " 'That',\n",
              " 'is',\n",
              " 'not',\n",
              " 'good',\n",
              " ',',\n",
              " 'This',\n",
              " 'is',\n",
              " 'better',\n",
              " '!',\n",
              " \"''\"]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Perform Sentence Tokenization"
      ],
      "metadata": {
        "id": "V_OZSpwN4sch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(text)\n",
        "textnew=\"Mr. Anil is talking lot. He will change his place.\"\n",
        "sent_tokenize(textnew)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCPQvFbK4l5h",
        "outputId": "4bc3db8d-6e38-4f3a-f737-b5b561c9a016"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr. Anil is talking lot.', 'He will change his place.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texth='''आर्टिफिशियल इंटेलिजेंस की शुरुआत साल 1950 में हुई थी। यह ह्यूमन कंप्यूटर इंटरेक्शन है।'''\n",
        "sent_tokenize(texth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJI9Ek4l5ZMp",
        "outputId": "91ef52be-2437-49e5-88b0-d7bb3f813e90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['आर्टिफिशियल इंटेलिजेंस की शुरुआत साल 1950 में हुई थी। यह ह्यूमन कंप्यूटर इंटरेक्शन है।']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Spacy"
      ],
      "metadata": {
        "id": "xXgMoygh5Bix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.hi import Hindi\n",
        "texth='''आर्टिफिशियल इंटेलिजेंस की शुरुआत साल 1950 में हुई थी। यह ह्यूमन कंप्यूटर इंटरेक्शन है।'''\n",
        "textnew=\"Mr. Anil is talking lot. He will change his place.\"\n",
        "nlp=English()\n",
        "doc=nlp(text)\n",
        "nlp.add_pipe('sentencizer')\n",
        "doc_1=nlp(texth)\n",
        "for i in doc_1.sents:\n",
        "  print(i.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGcwqIGh5APQ",
        "outputId": "e48b31d3-42f7-4e43-e1a3-cf338f66fa33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "आर्टिफिशियल इंटेलिजेंस की शुरुआत साल 1950 में हुई थी।\n",
            "यह ह्यूमन कंप्यूटर इंटरेक्शन है।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Perform Stemming"
      ],
      "metadata": {
        "id": "ShCY1pt56C0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Porter Stemmer"
      ],
      "metadata": {
        "id": "YaWeX9qG6R8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stemmer = nltk.PorterStemmer()\n",
        "def stemming(data):\n",
        "  d=dict()\n",
        "  for w in data.split():\n",
        "    d[w]=stemmer.stem(w)\n",
        "  return d"
      ],
      "metadata": {
        "id": "ecfh7dRF8-Kh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmng=stemming(\"computing world is computed compulsive\")\n",
        "print(stemmng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzlLTQ0-8_qQ",
        "outputId": "90bbfe0c-8d1c-4938-d3a9-2917393c1a13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computing': 'comput', 'world': 'world', 'is': 'is', 'computed': 'comput', 'compulsive': 'compuls'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using snowball stemmer"
      ],
      "metadata": {
        "id": "uJyfhLoM6U5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow=SnowballStemmer('english')\n",
        "for i in l:\n",
        "  print(snow.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXBFHaMH6Q2p",
        "outputId": "6f9799b2-96df-450d-b473-f987df5c371b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n",
            "eaten\n",
            "eat\n",
            "ate\n",
            "talk\n",
            "talk\n",
            "meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Lancaster stemmer"
      ],
      "metadata": {
        "id": "utw-x0Rg6fpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lanc=LancasterStemmer()\n",
        "for i in l:\n",
        "  print(lanc.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz5dplbK6dTH",
        "outputId": "e3bf26ac-5afe-40e2-dba1-e9251bd585e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n",
            "eat\n",
            "eat\n",
            "at\n",
            "talk\n",
            "talk\n",
            "meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Perform Lemmatization"
      ],
      "metadata": {
        "id": "zVj_gM9m6p24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "st=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "for i in l:\n",
        "  print(i, \" | \", st.lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aZBczmT6lIn",
        "outputId": "c25f2173-6f82-4ace-d668-290ec8406328"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eat  |  Eat\n",
            "Eats  |  Eats\n",
            "Eaten  |  Eaten\n",
            "Eating  |  Eating\n",
            "ate  |  ate\n",
            "Talk  |  Talk\n",
            "Talking  |  Talking\n",
            "meeting  |  meeting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization Using Spacy "
      ],
      "metadata": {
        "id": "c-B8I90M63Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"Eat Eats Eaten Eating ate Talk Talking meeting\")\n",
        "for i in doc:\n",
        "  print(i, \" | \", i.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3p1JLI961MX",
        "outputId": "d218ec0f-7bf3-4321-bb36-42da78227141"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eat  |  eat\n",
            "Eats  |  eat\n",
            "Eaten  |  Eaten\n",
            "Eating  |  Eating\n",
            "ate  |  eat\n",
            "Talk  |  talk\n",
            "Talking  |  talk\n",
            "meeting  |  meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Perform n grams"
      ],
      "metadata": {
        "id": "rot_wVq-7lIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dntbaxo97BVg",
        "outputId": "15c721f0-e884-4b7a-880b-4a5c7a9ceb70"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "afSDO2Ak9P0w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"i ate chocolate cake\"\n",
        "r=word_tokenize(text)"
      ],
      "metadata": {
        "id": "ZsHZ6g0I9P4_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "list(bigrams(r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cw_g9oC9P8R",
        "outputId": "7dcb90b5-9314-47ee-e00e-445f7f7e2c45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'ate'), ('ate', 'chocolate'), ('chocolate', 'cake')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import trigrams\n",
        "list(trigrams(r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-iFN8xP9XVw",
        "outputId": "93463e7c-d403-4add-8244-fc2457854db1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'ate', 'chocolate'), ('ate', 'chocolate', 'cake')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import everygrams\n",
        "list(everygrams(r,4,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i188vcnQ9Xc5",
        "outputId": "1d0f2e72-6aaf-439b-cd00-148636833a90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'ate', 'chocolate', 'cake')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Remove stopwords"
      ],
      "metadata": {
        "id": "lo_Kv_ym8RtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('english')\n",
        "#DataFrame.apply(Function_to_apply_to_each_row)\n",
        "def rem_stopword(data):\n",
        "  li=[]\n",
        "  for w in data.split():\n",
        "    if w not in stop_words:\n",
        "      li.append(w)\n",
        "  return \" \".join(li)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBMnWWUf8my3",
        "outputId": "908c8221-0344-4146-a277-f03783c34ec6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=\"The course i am studying is AI ML \"\n",
        "print(rem_stopword(data.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx5Nph4l8qzP",
        "outputId": "f189b73f-8ec4-4433-fbf5-93d4eb59fe28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "course studying ai ml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6jgVsOp8w8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}